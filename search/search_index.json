{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Languru","text":"<p>The general-purpose LLM app stacks deploy AI services quickly and (stupidly) simply.</p> <pre><code> _\n| |    __ _ _ __   __ _ _   _ _ __ _   _\n| |   / _` | '_ \\ / _` | | | | '__| | | |\n| |__| (_| | | | | (_| | |_| | |  | |_| |\n|_____\\__,_|_| |_|\\__, |\\__,_|_|   \\__,_|\n                  |___/\n</code></pre> <p> </p> <p>Documentation: Github Pages</p>"},{"location":"#install-languru","title":"Install <code>Languru</code>","text":"<pre><code>pip install languru\n\n# Install For LLM deployment.\npip install languru[all]\n\n# Install development dependencies.\npoetry install -E &lt;extras&gt; --with dev\n\n# Or just install all dependencies.\npoetry install -E all --with dev --with docs\n</code></pre>"},{"location":"#openai-clients","title":"OpenAI Clients","text":"<p>Supported OpenAI clients:</p> <ul> <li><code>openai.OpenAI</code></li> <li><code>openai.AzureOpenAI</code></li> <li><code>languru.openai_plugins.clients.anthropic.AnthropicOpenAI</code></li> <li><code>languru.openai_plugins.clients.google.GoogleOpenAI</code></li> <li><code>languru.openai_plugins.clients.groq.GroqOpenAI</code></li> <li><code>languru.openai_plugins.clients.pplx.PerplexityOpenAI</code></li> <li><code>languru.openai_plugins.clients.voyage.VoyageOpenAI</code></li> </ul>"},{"location":"#openai-server","title":"OpenAI Server","text":"<pre><code>languru server run  # Remember set all needed `api-key` for OpenAI clients.\n</code></pre> <p>Query LLM service, which is fully compatible with OpenAI APIs.</p> <pre><code>from openai import OpenAI\n\nclient = OpenAI(base_url=\"http://localhost:8682/v1\")\nres = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n)\nfor choice in res.choices:\n    print(f\"{choice.message.role}: {choice.message.content}\")\n# assistant: Hello! How can I assist you today?\n</code></pre> <p>Chat streaming:</p> <pre><code>client = OpenAI(base_url=\"http://localhost:8682/v1\")\nres = client.chat.completions.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n    stream=True,\n)\nfor chunk in res:\n    for choice in chunk.choices:\n        if choice.delta.content:\n            print(choice.delta.content, end=\"\", flush=True)\n            # Hello! How can I assist you today?\n</code></pre> <p>OpenAI plugins clients:</p> <pre><code>client = OpenAI(base_url=\"http://localhost:8682/v1\")\nres = client.chat.completions.create(\n    model=\"google/gemini-1.5-flash\",\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello!\"},\n    ],\n    stream=True,\n)\nfor choice in res.choices:\n    print(f\"{choice.message.role}: {choice.message.content}\")\n</code></pre>"},{"location":"#concepts","title":"Concepts","text":"<ul> <li>Openai Clients</li> <li>Data Model</li> <li>Prompts</li> <li>OpenAI Server</li> <li>Docker</li> </ul>"},{"location":"concepts/docker/","title":"Docker","text":"<p>Docker hub: https://hub.docker.com/r/dockhardman/languru/tags</p>"},{"location":"concepts/openai_server/","title":"OpenAI Server","text":"<p>The <code>languru.server</code> module is a Restful API server that is fully compatible with OpenAI APIs. You can run server in a simple command <code>languru server run</code>.</p>"},{"location":"concepts/data_model/","title":"DataModel","text":"<p>The <code>DataModel</code> class is a base class for defining data models in the <code>languru</code> package. It provides class methods for creating instances of the data model from content generated by OpenAI's language model, validating the generated data against the model's schema, and extracting the desired information.</p>"},{"location":"concepts/data_model/#introduction","title":"Introduction","text":"<p>The <code>DataModel</code> class is built on top of the <code>BaseModel</code> class from the <code>pydantic</code> library, which allows for easy definition and validation of data models. It leverages the power of OpenAI's language model to generate structured data based on a given content and a predefined schema.</p> <p>The main purpose of the <code>DataModel</code> class is to provide a convenient way to extract relevant information from unstructured text using OpenAI's language model and transform it into a structured format defined by the data model.</p>"},{"location":"concepts/data_model/#usage","title":"Usage","text":"<p>To use the <code>DataModel</code> class, you need to create a subclass that defines the desired fields and their types. Here's an example:</p> <pre><code>from languru.prompts.repositories.data_model import DataModel\n\nclass Person(DataModel):\n    name: str\n    age: int\n    email: str\n</code></pre> <p>Once you have defined your data model, you can use the provided class methods to create instances of the model from content generated by OpenAI's language model.</p>"},{"location":"concepts/data_model/#models_from_openai-method","title":"<code>models_from_openai</code> method","text":"<p>The <code>models_from_openai</code> class method allows you to create multiple instances of the data model from the given content. It takes the following parameters:</p> <ul> <li><code>content</code>: The unstructured text content from which to extract the data.</li> <li><code>client</code>: An instance of the <code>OpenAI</code> client for making API calls.</li> <li><code>model</code>: The name of the OpenAI language model to use (default: \"gpt-3.5-turbo\").</li> <li><code>**kwargs</code>: Additional keyword arguments to pass to the OpenAI API.</li> </ul> <p>The method returns a list of instances of the data model, extracted from the generated content.</p> <p>Example usage:</p> <pre><code>from openai import OpenAI\nfrom languru.prompts.repositories.data_model import DataModel\n\nclass Person(DataModel):\n    name: str\n    age: int\n    email: str\n\ncontent = \"John Doe is 30 years old. His email is john@example.com.\"\nclient = OpenAI(api_key=\"your_api_key\")\n\npeople = Person.models_from_openai(content, client)\nfor person in people:\n    print(person.name, person.age, person.email)\n</code></pre>"},{"location":"concepts/data_model/#model_from_openai-method","title":"<code>model_from_openai</code> method","text":"<p>The <code>model_from_openai</code> class method is similar to <code>models_from_openai</code>, but it returns a single instance of the data model. If multiple instances are extracted from the content, only the first one is returned.</p> <p>Example usage:</p> <pre><code>person = Person.model_from_openai(content, client)\nprint(person.name, person.age, person.email)\n</code></pre>"},{"location":"concepts/data_model/#validation","title":"Validation","text":"<p>The <code>DataModel</code> class automatically validates the generated data against the defined schema using the <code>pydantic</code> library. If the validation fails, a <code>ValidationError</code> is raised, indicating that the generated data does not match the expected format.</p>"},{"location":"concepts/data_model/#error-handling","title":"Error Handling","text":"<p>The <code>DataModel</code> class provides error handling for the following scenarios:</p> <ul> <li>If no JSON code block is found in the generated content, a <code>ValueError</code> is raised with an appropriate error message.</li> <li>If the generated data fails to validate against the defined schema, a <code>ValidationError</code> is raised, indicating the validation errors.</li> </ul>"},{"location":"concepts/data_model/#customization","title":"Customization","text":"<p>You can customize the behavior of the <code>DataModel</code> class by overriding its methods in your subclass. For example, you can modify the <code>model_json_schema</code> method to provide a custom schema for your data model.</p> <p>Additionally, you can extend the functionality of the <code>DataModel</code> class by adding your own methods and properties to your subclass.</p>"},{"location":"concepts/data_model/#conclusion","title":"Conclusion","text":"<p>The <code>DataModel</code> class provides a powerful and flexible way to extract structured data from unstructured text using OpenAI's language model. By defining your data models as subclasses of <code>DataModel</code>, you can easily create instances of the models from generated content and validate the data against the defined schema.</p>"},{"location":"concepts/openai_clients/","title":"<code>languru.openai_plugins.clients</code> Module Documentation","text":""},{"location":"concepts/openai_clients/#overview","title":"Overview","text":"<p>The <code>languru.openai_plugins.clients</code> module provides a set of client implementations for various AI service providers, offering OpenAI-compatible interfaces. This module enables seamless integration with different AI services while maintaining a consistent API.</p>"},{"location":"concepts/openai_clients/#supported-clients","title":"Supported Clients","text":"<ul> <li>AnthropicOpenAI</li> <li>GoogleOpenAI</li> <li>GroqOpenAI</li> <li>PerplexityOpenAI</li> <li>VoyageOpenAI</li> </ul>"},{"location":"concepts/openai_clients/#architecture","title":"Architecture","text":"<pre><code>graph TD\n    A[OpenAI] --&gt; B[AnthropicOpenAI]\n    A --&gt; C[GoogleOpenAI]\n    A --&gt; D[GroqOpenAI]\n    A --&gt; E[PerplexityOpenAI]\n    A --&gt; F[VoyageOpenAI]</code></pre>"},{"location":"concepts/openai_clients/#key-features","title":"Key Features","text":"<ol> <li>Unified Interface: All clients extend the base OpenAI client, providing a consistent API across different providers.</li> <li>Customized Implementations: Each client adapts the OpenAI interface to the specific requirements of its respective AI service.</li> <li>Flexible Configuration: Clients support various initialization parameters, including API keys and custom settings.</li> </ol>"},{"location":"concepts/openai_clients/#usage-example","title":"Usage Example","text":"<pre><code>from languru.openai_plugins.clients import AnthropicOpenAI\n\nclient = AnthropicOpenAI(api_key=\"your_api_key_here\")\nresponse = client.chat.completions.create(\n    model=\"claude-2\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n)\nprint(response.choices.message.content)\n</code></pre>"},{"location":"concepts/openai_clients/#client-specific-notes","title":"Client-Specific Notes","text":""},{"location":"concepts/openai_clients/#anthropicopenai","title":"AnthropicOpenAI","text":"<ul> <li>Implements Anthropic's API with OpenAI-compatible interfaces.</li> <li>Supports streaming responses.</li> </ul>"},{"location":"concepts/openai_clients/#googleopenai","title":"GoogleOpenAI","text":"<ul> <li>Adapts Google's GenerativeAI API to the OpenAI interface.</li> <li>Includes support for embeddings.</li> </ul>"},{"location":"concepts/openai_clients/#groqopenai","title":"GroqOpenAI","text":"<ul> <li>Provides an OpenAI-compatible interface for Groq's API.</li> <li>Implements both streaming and non-streaming chat completions.</li> </ul>"},{"location":"concepts/openai_clients/#perplexityopenai","title":"PerplexityOpenAI","text":"<ul> <li>Offers a minimal OpenAI-compatible interface for Perplexity AI.</li> <li>Focuses on model listing and retrieval.</li> </ul>"},{"location":"concepts/openai_clients/#voyageopenai","title":"VoyageOpenAI","text":"<ul> <li>Implements OpenAI-compatible embeddings for Voyage AI.</li> <li>Supports various Voyage-specific embedding models.</li> </ul>"},{"location":"concepts/openai_clients/#error-handling","title":"Error Handling","text":"<p>Clients implement custom error handling to map provider-specific errors to OpenAI-compatible exceptions, ensuring consistent error management across different services.</p>"},{"location":"concepts/openai_clients/#conclusion","title":"Conclusion","text":"<p>The <code>languru.openai_plugins.clients</code> module simplifies integration with multiple AI services by providing a unified, OpenAI-compatible interface. This approach allows developers to easily switch between different AI providers while maintaining consistent code structure and API usage.</p>"},{"location":"concepts/openai_clients/anthropic/","title":"Anthropic OpenAI","text":""},{"location":"concepts/openai_clients/anthropic/#introduction","title":"Introduction","text":"<p>The <code>languru.openai_plugins.clients.anthropic</code> module is designed to interact with the Anthropic AI platform, providing functionality for chat completions and model management. This documentation aims to provide a clear and concise overview of the module's functionality, ensuring that developers and users can quickly understand and utilize its capabilities.</p>"},{"location":"concepts/openai_clients/anthropic/#module-overview","title":"Module Overview","text":"<p>The <code>languru.openai_plugins.clients.anthropic</code> module consists of several key components:</p> <ul> <li>AnthropicChatCompletions: Handles chat completion requests and streaming functionalities.</li> <li>AnthropicModels: Manages model retrieval and listing.</li> <li>AnthropicOpenAI: The main class that encapsulates the Anthropic client and its functionalities.</li> </ul>"},{"location":"concepts/openai_clients/anthropic/#anthropicchatcompletions","title":"AnthropicChatCompletions","text":"<pre><code>classDiagram\n    class AnthropicChatCompletions {\n        +create(messages, model, ...) ChatCompletion | Stream[ChatCompletionChunk]\n        +_create(messages, model, ...) ChatCompletion\n        +_create_stream(messages, model, ...) Stream[ChatCompletionChunk]\n    }</code></pre> <ul> <li>create: Creates a chat completion. It can return either a <code>ChatCompletion</code> object or a stream of <code>ChatCompletionChunk</code> objects depending on the <code>stream</code> parameter.</li> <li>_create: An internal method that handles the creation of a chat completion without streaming.</li> <li>_create_stream: An internal method that handles the creation of a chat completion stream.</li> </ul>"},{"location":"concepts/openai_clients/anthropic/#anthropicmodels","title":"AnthropicModels","text":"<pre><code>classDiagram\n    class AnthropicModels {\n        +retrieve(model, ...) Model\n        +list(...) SyncPage[Model]\n    }</code></pre> <ul> <li>retrieve: Retrieves a specific model by its ID.</li> <li>list: Lists all supported models.</li> </ul>"},{"location":"concepts/openai_clients/anthropic/#anthropicopenai","title":"AnthropicOpenAI","text":"<pre><code>classDiagram\n    class AnthropicOpenAI {\n        +__init__(api_key, ...) void\n        +chat: AnthropicChat\n        +models: AnthropicModels\n        +anthropic_client: anthropic.Anthropic\n    }</code></pre> <ul> <li>*init*: Initializes the AnthropicOpenAI client with an API key.</li> <li>chat: An instance of <code>AnthropicChat</code> for handling chat-related functionalities.</li> <li>models: An instance of <code>AnthropicModels</code> for managing models.</li> <li>anthropic_client: The underlying Anthropic client.</li> </ul>"},{"location":"concepts/openai_clients/anthropic/#usage-examples","title":"Usage Examples","text":""},{"location":"concepts/openai_clients/anthropic/#creating-a-chat-completion","title":"Creating a Chat Completion","text":"<pre><code>from languru.openai_plugins.clients.anthropic import AnthropicOpenAI\n\n# Initialize the AnthropicOpenAI client\nclient = AnthropicOpenAI(api_key=\"your_api_key\")\n\n# Create a chat completion\ncompletion = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    model=\"claude-v1\",\n)\n\nprint(completion)\n</code></pre>"},{"location":"concepts/openai_clients/anthropic/#streaming-a-chat-completion","title":"Streaming a Chat Completion","text":"<pre><code>from languru.openai_plugins.clients.anthropic import AnthropicOpenAI\n\n# Initialize the AnthropicOpenAI client\nclient = AnthropicOpenAI(api_key=\"your_api_key\")\n\n# Create a chat completion stream\nstream = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    model=\"claude-v1\",\n    stream=True,\n)\n\nfor chunk in stream:\n    print(chunk)\n</code></pre>"},{"location":"concepts/openai_clients/anthropic/#retrieving-a-model","title":"Retrieving a Model","text":"<pre><code>from languru.openai_plugins.clients.anthropic import AnthropicOpenAI\n\n# Initialize the AnthropicOpenAI client\nclient = AnthropicOpenAI(api_key=\"your_api_key\")\n\n# Retrieve a model\nmodel = client.models.retrieve(\"claude-v1\")\n\nprint(model)\n</code></pre>"},{"location":"concepts/openai_clients/anthropic/#listing-models","title":"Listing Models","text":"<pre><code>from languru.openai_plugins.clients.anthropic import AnthropicOpenAI\n\n# Initialize the AnthropicOpenAI client\nclient = AnthropicOpenAI(api_key=\"your_api_key\")\n\n# List all supported models\nmodels = client.models.list()\n\nfor model in models.data:\n    print(model)\n</code></pre>"},{"location":"concepts/openai_clients/anthropic/#conclusion","title":"Conclusion","text":"<p>The <code>languru.openai_plugins.clients.anthropic</code> module provides a comprehensive interface for interacting with the Anthropic AI platform. By following this documentation, developers and users can quickly understand and utilize the module's functionalities to create chat completions and manage models.</p>"},{"location":"concepts/openai_clients/google/","title":"Google OpenAI","text":""},{"location":"concepts/openai_clients/google/#overview-of-languruopenai_pluginsclientsgoogle-module","title":"Overview of <code>languru.openai_plugins.clients.google</code> Module","text":"<p>The <code>languru.openai_plugins.clients.google</code> module is designed to integrate Google's GenAI capabilities with the OpenAI framework. It provides functionalities for chat completions, embeddings, and model management.</p>"},{"location":"concepts/openai_clients/google/#module-structure","title":"Module Structure","text":"<pre><code>graph LR\n    GoogleChatCompletions --&gt; GoogleChat\n    GoogleModels --&gt; GoogleOpenAI\n    GoogleEmbeddings --&gt; GoogleOpenAI\n    GoogleOpenAI --&gt; GoogleChat</code></pre>"},{"location":"concepts/openai_clients/google/#classes-and-methods","title":"Classes and Methods","text":""},{"location":"concepts/openai_clients/google/#googlechatcompletions","title":"GoogleChatCompletions","text":"<ul> <li>Class Description: This class handles chat completion tasks using Google GenAI.</li> <li>Methods:<ul> <li><code>create</code>: Creates a chat completion.</li> <li><code>_create</code>: Internal method for creating a chat completion.</li> <li><code>_create_stream</code>: Internal method for creating a chat completion stream.</li> <li><code>generator_generate_content_chunks</code>: Generates chat completion response in chunks.</li> </ul> </li> </ul>"},{"location":"concepts/openai_clients/google/#googlemodels","title":"GoogleModels","text":"<ul> <li>Class Description: This class manages models for Google GenAI.</li> <li>Methods:<ul> <li><code>retrieve</code>: Retrieves a model.</li> <li><code>list</code>: Lists available models.</li> </ul> </li> </ul>"},{"location":"concepts/openai_clients/google/#googleembeddings","title":"GoogleEmbeddings","text":"<ul> <li>Class Description: This class handles embedding tasks using Google GenAI.</li> <li>Methods:<ul> <li><code>create</code>: Creates an embedding.</li> </ul> </li> </ul>"},{"location":"concepts/openai_clients/google/#googleopenai","title":"GoogleOpenAI","text":"<ul> <li>Class Description: This class serves as the main entry point for Google GenAI integration with OpenAI.</li> <li>Methods:<ul> <li><code>__init__</code>: Initializes the GoogleOpenAI instance.</li> </ul> </li> </ul>"},{"location":"concepts/openai_clients/google/#usage-examples","title":"Usage Examples","text":"<pre><code># Initialize GoogleOpenAI\ngoogle_openai = GoogleOpenAI(api_key=\"YOUR_API_KEY\")\n\n# Create a chat completion\ncompletion = google_openai.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello\"}],\n    model=\"YOUR_MODEL_NAME\"\n)\n\n# Retrieve a model\nmodel = google_openai.models.retrieve(model=\"YOUR_MODEL_NAME\")\n\n# Create an embedding\nembedding = google_openai.embeddings.create(\n    input=\"YOUR_INPUT_TEXT\",\n    model=\"YOUR_MODEL_NAME\"\n)\n</code></pre>"},{"location":"concepts/openai_clients/google/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>CredentialsNotProvided: Make sure to provide a valid Google GenAI API key.</li> <li>GoogleNotFound: Check if the model or resource exists.</li> </ul>"},{"location":"concepts/openai_clients/google/#conclusion","title":"Conclusion","text":"<p>The <code>languru.openai_plugins.clients.google</code> module provides a comprehensive integration of Google GenAI with OpenAI. By following this documentation, you can effectively utilize the module's functionalities for chat completions, embeddings, and model management.</p>"},{"location":"concepts/openai_clients/groq/","title":"Groq OpenAI","text":""},{"location":"concepts/openai_clients/groq/#introduction","title":"Introduction","text":"<p>The <code>languru.openai_plugins.clients.groq</code> module is designed to integrate with the Groq AI platform, providing functionalities for chat completions and model management. This documentation aims to provide a clear and concise overview of the module's capabilities and usage.</p>"},{"location":"concepts/openai_clients/groq/#module-overview","title":"Module Overview","text":"<p>The module consists of several classes and functions that facilitate interaction with the Groq AI platform. The main components include:</p> <ul> <li>GroqChatCompletions: Handles chat completion requests, both synchronous and asynchronous (streaming).</li> <li>GroqModels: Manages model retrieval and listing.</li> <li>GroqOpenAI: The base class that encapsulates the Groq client and provides access to chat and model functionalities.</li> </ul>"},{"location":"concepts/openai_clients/groq/#groqchatcompletions","title":"GroqChatCompletions","text":""},{"location":"concepts/openai_clients/groq/#methods","title":"Methods","text":"<ul> <li>create: Creates a chat completion. This method can operate in both synchronous and asynchronous modes depending on the <code>stream</code> parameter.<ul> <li>Parameters:<ul> <li><code>messages</code>: An iterable of <code>ChatCompletionMessageParam</code> objects.</li> <li><code>model</code>: The model to use, specified as a string or <code>ChatModel</code> object.</li> <li><code>stream</code>: A boolean indicating whether to use streaming (asynchronous) mode.</li> <li>Additional parameters for fine-tuning the completion, such as <code>frequency_penalty</code>, <code>logit_bias</code>, <code>max_tokens</code>, etc.</li> </ul> </li> <li>Returns: A <code>ChatCompletion</code> object in synchronous mode or a <code>Stream[ChatCompletionChunk]</code> in asynchronous mode.</li> </ul> </li> </ul>"},{"location":"concepts/openai_clients/groq/#groqmodels","title":"GroqModels","text":""},{"location":"concepts/openai_clients/groq/#methods_1","title":"Methods","text":"<ul> <li>retrieve: Retrieves a specific model.<ul> <li>Parameters:<ul> <li><code>model</code>: The ID of the model to retrieve.</li> </ul> </li> <li>Returns: A <code>Model</code> object.</li> </ul> </li> <li>list: Lists available models.<ul> <li>Returns: A <code>SyncPage[Model]</code> object containing a list of models.</li> </ul> </li> </ul>"},{"location":"concepts/openai_clients/groq/#groqopenai","title":"GroqOpenAI","text":""},{"location":"concepts/openai_clients/groq/#initialization","title":"Initialization","text":"<ul> <li>*init*: Initializes the GroqOpenAI client.<ul> <li>Parameters:<ul> <li><code>api_key</code>: The API key for the Groq platform.</li> <li>Additional keyword arguments for client configuration.</li> </ul> </li> </ul> </li> </ul>"},{"location":"concepts/openai_clients/groq/#usage-examples","title":"Usage Examples","text":""},{"location":"concepts/openai_clients/groq/#creating-a-chat-completion","title":"Creating a Chat Completion","text":"<pre><code>from languru.openai_plugins.clients.groq import GroqOpenAI\n\n# Initialize the client\nclient = GroqOpenAI(api_key=\"your_api_key\")\n\n# Create a chat completion\ncompletion = client.chat.completions.create(\n    messages=[{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n    model=\"groq-1\",\n    stream=False\n)\n\nprint(completion)\n</code></pre>"},{"location":"concepts/openai_clients/groq/#listing-models","title":"Listing Models","text":"<pre><code>from languru.openai_plugins.clients.groq import GroqOpenAI\n\n# Initialize the client\nclient = GroqOpenAI(api_key=\"your_api_key\")\n\n# List models\nmodels = client.models.list()\n\nfor model in models.data:\n    print(model.id)\n</code></pre>"},{"location":"concepts/openai_clients/groq/#visual-overview","title":"Visual Overview","text":"<pre><code>graph LR\n    A[GroqOpenAI] --&gt;|chat| B[GroqChat]\n    A --&gt;|models| C[GroqModels]\n    B --&gt;|completions| D[GroqChatCompletions]\n    C --&gt;|retrieve/list| E[Models]\n    D --&gt;|create| F[ChatCompletion]\n    style A fill:#f9f,stroke:#333,stroke-width:2px\n    style B fill:#f9f,stroke:#333,stroke-width:2px\n    style C fill:#f9f,stroke:#333,stroke-width:2px\n    style D fill:#f9f,stroke:#333,stroke-width:2px\n    style E fill:#f9f,stroke:#333,stroke-width:2px\n    style F fill:#f9f,stroke:#333,stroke-width:2px</code></pre>"},{"location":"concepts/openai_clients/groq/#conclusion","title":"Conclusion","text":"<p>The <code>languru.openai_plugins.clients.groq</code> module provides a robust interface for interacting with the Groq AI platform, enabling developers to create chat completions and manage models efficiently. This documentation aims to provide a quick and comprehensive guide to using the module, ensuring that developers can integrate its functionalities into their applications with ease.</p>"},{"location":"concepts/openai_clients/pplx/","title":"Perplexity OpenAI","text":""},{"location":"concepts/openai_clients/pplx/#introduction","title":"Introduction","text":"<p>The <code>languru.openai_plugins.clients.pplx</code> module is a software component designed to interact with the Perplexity API, providing functionality for retrieving and listing models. The purpose of this module is to facilitate the integration of Perplexity's language model capabilities into larger applications.</p>"},{"location":"concepts/openai_clients/pplx/#analysis","title":"Analysis","text":"<p>Upon reviewing the provided code, the module appears to be well-structured and follows standard Python conventions. The use of type hints, docstrings, and clear variable names makes the code readable and understandable. However, there are a few areas that could be improved:</p> <ul> <li>The <code>PerplexityModels</code> class has a hardcoded list of supported models. This could be made more flexible by allowing users to dynamically register or unregister models.</li> <li>The <code>retrieve</code> method does not handle potential exceptions that may occur when making API requests. Adding error handling mechanisms would improve the module's robustness.</li> <li>The <code>list</code> method returns a <code>SyncPage</code> object, which may not be immediately clear to users. Providing additional documentation or examples on how to work with this object would be beneficial.</li> </ul>"},{"location":"concepts/openai_clients/pplx/#suggestions","title":"Suggestions","text":"<p>To improve the <code>languru.openai_plugins.clients.pplx</code> module, the following suggestions are proposed:</p> <ul> <li>Introduce a registration mechanism for supported models, allowing users to add or remove models dynamically.</li> <li>Implement error handling for API requests in the <code>retrieve</code> method, providing informative error messages and potentially retrying failed requests.</li> <li>Enhance the documentation for the <code>list</code> method, providing examples or explanations on how to work with the returned <code>SyncPage</code> object.</li> </ul>"},{"location":"concepts/openai_clients/pplx/#documentation-strategy","title":"Documentation Strategy","text":"<p>To assist the user in writing documentation more easily, the following strategy is proposed:</p> <ul> <li>Utilize the Markdown format for documentation, allowing for easy formatting and readability.</li> <li>Provide clear and concise explanations of each module component, including classes, methods, and functions.</li> <li>Include examples and code snippets to illustrate the usage of each component.</li> <li>Use headers and subheaders to organize the documentation and make it easier to navigate.</li> </ul>"},{"location":"concepts/openai_clients/pplx/#documentation","title":"Documentation","text":""},{"location":"concepts/openai_clients/pplx/#overview","title":"Overview","text":"<p>The <code>languru.openai_plugins.clients.pplx</code> module provides functionality for interacting with the Perplexity API, allowing users to retrieve and list models.</p>"},{"location":"concepts/openai_clients/pplx/#usage","title":"Usage","text":"<p>To use the module, first import it and initialize the <code>PerplexityOpenAI</code> class with your API key:</p> <pre><code>from languru.openai_plugins.clients.pplx import PerplexityOpenAI\n\napi_key = \"your_api_key_here\"\npplx = PerplexityOpenAI(api_key=api_key)\n</code></pre>"},{"location":"concepts/openai_clients/pplx/#retrieving-models","title":"Retrieving Models","text":"<p>To retrieve a specific model, use the <code>retrieve</code> method:</p> <pre><code>model = pplx.models.retrieve(\"model_id\")\n</code></pre>"},{"location":"concepts/openai_clients/pplx/#listing-models","title":"Listing Models","text":"<p>To list all available models, use the <code>list</code> method:</p> <pre><code>models = pplx.models.list()\n</code></pre>"},{"location":"concepts/openai_clients/pplx/#error-handling","title":"Error Handling","text":"<p>The <code>retrieve</code> method may raise exceptions if the API request fails. To handle these exceptions, use a try-except block:</p> <pre><code>try:\n    model = pplx.models.retrieve(\"model_id\")\nexcept openai.NotFoundError as e:\n    print(f\"Error: {e}\")\n</code></pre>"},{"location":"concepts/openai_clients/pplx/#syncpage-object","title":"SyncPage Object","text":"<p>The <code>list</code> method returns a <code>SyncPage</code> object, which contains a list of models. To access the models, use the <code>data</code> attribute:</p> <pre><code>models = pplx.models.list()\nfor model in models.data:\n    print(model.id)\n</code></pre>"},{"location":"concepts/openai_clients/pplx/#mermaid-diagram","title":"Mermaid Diagram","text":"<p>The following Mermaid diagram illustrates the module's functionality:</p> <pre><code>graph TD\n    A[User] --&gt;|initialize| B[PerplexityOpenAI]\n    B --&gt;|retrieve| C[Model]\n    B --&gt;|list| D[SyncPage]\n    D --&gt;|data| E[Models]</code></pre>"},{"location":"concepts/openai_clients/pplx/#conclusion","title":"Conclusion","text":"<p>In conclusion, the <code>languru.openai_plugins.clients.pplx</code> module is a well-structured and functional component that provides useful functionality for interacting with the Perplexity API. By implementing the suggested improvements and following the proposed documentation strategy, the module can become even more effective and user-friendly. Proper documentation is essential for ensuring that users can understand and utilize the module's capabilities, and it is hoped that this documentation will assist in achieving this goal.</p>"},{"location":"concepts/openai_clients/voyage/","title":"Voyage OpenAI","text":""},{"location":"concepts/openai_clients/voyage/#introduction","title":"Introduction","text":"<p>The <code>languru.openai_plugins.clients.voyage</code> module is designed to interact with the Voyage AI platform, providing functionalities for model management and embedding creation. This documentation aims to provide a clear and concise overview of the module's functionality and usage.</p>"},{"location":"concepts/openai_clients/voyage/#module-overview","title":"Module Overview","text":"<p>The <code>languru.openai_plugins.clients.voyage</code> module consists of several classes:</p> <ul> <li>VoyageModels: Handles model retrieval and validation.</li> <li>VoyageEmbeddings: Creates embeddings for input text or texts.</li> <li>VoyageOpenAI: Manages API interactions and client initialization.</li> </ul>"},{"location":"concepts/openai_clients/voyage/#voyagemodels","title":"VoyageModels","text":"<p>The <code>VoyageModels</code> class provides methods for retrieving and listing supported models.</p>"},{"location":"concepts/openai_clients/voyage/#retrieve-model","title":"Retrieve Model","text":"<pre><code>def retrieve(self, model: str, *, extra_headers: Headers | None = None, extra_query: Query | None = None, extra_body: Body | None = None, timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN, **kwargs,) -&gt; \"Model\":\n    \"\"\"\n    Retrieve a specific model by its ID.\n\n    Args:\n        model (str): The ID of the model to retrieve.\n        extra_headers (Headers | None, optional): Additional headers for the request. Defaults to None.\n        extra_query (Query | None, optional): Additional query parameters for the request. Defaults to None.\n        extra_body (Body | None, optional): Additional body data for the request. Defaults to None.\n        timeout (float | httpx.Timeout | None | NotGiven, optional): Timeout for the request. Defaults to NOT_GIVEN.\n\n    Returns:\n        Model: The retrieved model.\n    \"\"\"\n</code></pre>"},{"location":"concepts/openai_clients/voyage/#list-models","title":"List Models","text":"<pre><code>def list(self, *, extra_headers: Headers | None = None, extra_query: Query | None = None, extra_body: Body | None = None, timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN, **kwargs,) -&gt; \"SyncPage[Model]\":\n    \"\"\"\n    List all supported models.\n\n    Args:\n        extra_headers (Headers | None, optional): Additional headers for the request. Defaults to None.\n        extra_query (Query | None, optional): Additional query parameters for the request. Defaults to None.\n        extra_body (Body | None, optional): Additional body data for the request. Defaults to None.\n        timeout (float | httpx.Timeout | None | NotGiven, optional): Timeout for the request. Defaults to NOT_GIVEN.\n\n    Returns:\n        SyncPage[Model]: A page of supported models.\n    \"\"\"\n</code></pre>"},{"location":"concepts/openai_clients/voyage/#voyageembeddings","title":"VoyageEmbeddings","text":"<p>The <code>VoyageEmbeddings</code> class provides a method for creating embeddings.</p>"},{"location":"concepts/openai_clients/voyage/#create-embedding","title":"Create Embedding","text":"<pre><code>def create(self, *, input: Union[str, List[str], Iterable[int], Iterable[Iterable[int]]], model: Text, dimensions: int | NotGiven = NOT_GIVEN, encoding_format: Literal[\"float\", \"base64\"] | NotGiven = NOT_GIVEN, user: str | NotGiven = NOT_GIVEN, extra_headers: Headers | None = None, extra_query: Query | None = None, extra_body: Body | None = None, timeout: float | httpx.Timeout | None | NotGiven = NOT_GIVEN,) -&gt; CreateEmbeddingResponse:\n    \"\"\"\n    Create an embedding for the input text or texts.\n\n    Args:\n        input (Union[str, List[str], Iterable[int], Iterable[Iterable[int]]]): The input text or texts to create embeddings for.\n        model (Text): The model to use for creating embeddings.\n        dimensions (int | NotGiven, optional): The number of dimensions for the embeddings. Defaults to NOT_GIVEN.\n        encoding_format (Literal[\"float\", \"base64\"] | NotGiven, optional): The encoding format for the embeddings. Defaults to NOT_GIVEN.\n        user (str | NotGiven, optional): The user to create embeddings for. Defaults to NOT_GIVEN.\n        extra_headers (Headers | None, optional): Additional headers for the request. Defaults to None.\n        extra_query (Query | None, optional): Additional query parameters for the request. Defaults to None.\n        extra_body (Body | None, optional): Additional body data for the request. Defaults to None.\n        timeout (float | httpx.Timeout | None | NotGiven, optional): Timeout for the request. Defaults to NOT_GIVEN.\n\n    Returns:\n        CreateEmbeddingResponse: The created embedding response.\n    \"\"\"\n</code></pre>"},{"location":"concepts/openai_clients/voyage/#voyageopenai","title":"VoyageOpenAI","text":"<p>The <code>VoyageOpenAI</code> class manages API interactions and client initialization.</p>"},{"location":"concepts/openai_clients/voyage/#initialize-client","title":"Initialize Client","text":"<pre><code>def __init__(self, *, api_key: Optional[Text] = None, **kwargs):\n    \"\"\"\n    Initialize the VoyageOpenAI client.\n\n    Args:\n        api_key (Optional[Text], optional): The API key to use for authentication. Defaults to None.\n    \"\"\"\n</code></pre>"},{"location":"concepts/openai_clients/voyage/#usage-examples","title":"Usage Examples","text":""},{"location":"concepts/openai_clients/voyage/#retrieving-a-model","title":"Retrieving a Model","text":"<pre><code>voyage_models = VoyageModels()\nmodel = voyage_models.retrieve(\"model-id\")\nprint(model)\n</code></pre>"},{"location":"concepts/openai_clients/voyage/#creating-an-embedding","title":"Creating an Embedding","text":"<pre><code>voyage_embeddings = VoyageEmbeddings()\nembedding = voyage_embeddings.create(input=\"Hello, World!\", model=\"model-id\")\nprint(embedding)\n</code></pre>"},{"location":"concepts/openai_clients/voyage/#initializing-the-client","title":"Initializing the Client","text":"<pre><code>voyage_openai = VoyageOpenAI(api_key=\"your-api-key\")\nprint(voyage_openai)\n</code></pre>"},{"location":"concepts/openai_clients/voyage/#conclusion","title":"Conclusion","text":"<p>The <code>languru.openai_plugins.clients.voyage</code> module provides essential functionalities for interacting with the Voyage AI platform. By following this documentation, developers can easily integrate the module into their projects and utilize its features for model management and embedding creation.</p>"},{"location":"concepts/prompts/","title":"Prompts","text":""},{"location":"concepts/prompts/#prompttemplate","title":"PromptTemplate","text":""},{"location":"concepts/prompts/prompt_template/","title":"PromptTemplate","text":"<p><code>PromptTemplate</code> is a Python class that provides a convenient way to create and manage prompt templates for generating text using language models. It allows you to define a prompt with placeholders for variables, which can be easily substituted with actual values when needed. The class also supports managing a list of messages and formatting them with the prompt variables.</p>"},{"location":"concepts/prompts/prompt_template/#introduction","title":"Introduction","text":"<p>The <code>PromptTemplate</code> class is designed to simplify the process of creating and using prompt templates in natural language processing tasks, particularly when working with language models like OpenAI's GPT. It provides a flexible and intuitive interface for defining prompts, managing prompt variables, and formatting messages.</p> <p>Key features of <code>PromptTemplate</code> include:</p> <ul> <li>Defining prompts with placeholders for variables</li> <li>Managing a dictionary of prompt variables</li> <li>Formatting prompts with actual values</li> <li>Maintaining a list of messages associated with the prompt</li> <li>Customizing the delimiter characters for placeholders</li> <li>Supporting different roles for messages (system, user, assistant)</li> </ul>"},{"location":"concepts/prompts/prompt_template/#usage","title":"Usage","text":"<p>To use the <code>PromptTemplate</code> class, you first need to instantiate it with a prompt string and optional arguments. Here's an example:</p> <pre><code>from languru.types.chat.completions import PromptTemplate\n\nprompt = \"Hello {name}, how are you doing today?\"\nmessages = [{\"role\": \"user\", \"content\": \"I'm doing well, thanks for asking!\"}]\n\ntemplate = PromptTemplate(prompt, prompt_vars={\"name\": \"John\"}, messages=messages)\n</code></pre> <p>In this example, we create a <code>PromptTemplate</code> instance with a prompt that includes a placeholder <code>{name}</code>, a dictionary of prompt variables with a default value for <code>name</code>, and a list of messages.</p> <p>You can format the prompt with actual values using the <code>format()</code> method:</p> <pre><code>formatted_prompt = template.format(name=\"Alice\")\nprint(formatted_prompt)\n# Output: Hello Alice, how are you doing today?\n</code></pre> <p>The <code>format()</code> method replaces the placeholders in the prompt with the provided values.</p> <p>You can also format the messages associated with the prompt using the <code>format_messages()</code> method:</p> <pre><code>formatted_messages = template.format_messages(prompt_vars={\"name\": \"Bob\"})\nprint(formatted_messages)\n# Output: [\n#   {\"role\": \"system\", \"content\": \"Hello Bob, how are you doing today?\"},\n#   {\"role\": \"user\", \"content\": \"I'm doing well, thanks for asking!\"}\n# ]\n</code></pre> <p>The <code>format_messages()</code> method applies the prompt variables to the messages and returns a list of formatted messages.</p>"},{"location":"concepts/prompts/prompt_template/#customization","title":"Customization","text":"<p><code>PromptTemplate</code> allows you to customize various aspects of the prompt and messages:</p> <ul> <li><code>prompt_vars_update()</code>: Update the prompt variables dictionary</li> <li><code>prompt_vars_drop()</code>: Remove specific prompt variables from the dictionary</li> <li><code>prompt_placeholders()</code>: Get a list of placeholders in the prompt</li> <li><code>role_system</code>, <code>role_user</code>, <code>role_assistant</code>: Customize the role names for messages</li> </ul> <p>You can refer to the class documentation for more details on these methods and attributes.</p>"},{"location":"concepts/prompts/prompt_template/#conclusion","title":"Conclusion","text":"<p>The <code>PromptTemplate</code> class provides a powerful and flexible way to manage prompt templates and generate formatted prompts and messages. It simplifies the process of working with language models and allows you to focus on the high-level logic of your application. By leveraging the features of <code>PromptTemplate</code>, you can easily create and customize prompts, substitute variables, and manage messages associated with the prompts.</p>"},{"location":"server/audio/","title":"Audio API","text":""},{"location":"server/audio/#introduction","title":"Introduction","text":"<p>The <code>languru.server.api.v1.audio</code> module provides RESTful API routers for application use, specifically handling audio-related requests and responses. This documentation aims to provide a clear and concise guide on how to use these APIs effectively.</p>"},{"location":"server/audio/#api-endpoints","title":"API Endpoints","text":""},{"location":"server/audio/#audio-speech","title":"Audio Speech","text":""},{"location":"server/audio/#post-audiospeech","title":"POST /audio/speech","text":"<p>This endpoint generates audio speech based on the provided request.</p> <p>Request Body</p> <ul> <li><code>model</code>: The model to use for speech generation.</li> <li><code>voice</code>: The voice to use for speech generation.</li> <li><code>input</code>: The text to generate speech for.</li> </ul> <p>Response</p> <ul> <li>A streaming response containing the generated audio speech.</li> </ul> <p>Example</p> <pre><code>curl -X POST \\\n  http://localhost:8000/audio/speech \\\n  -H 'Content-Type: application/json' \\\n  -d '{\"model\": \"tts-1\", \"voice\": \"alloy\", \"input\": \"The quick brown fox jumped over the lazy dog.\"}'\n</code></pre>"},{"location":"server/audio/#audio-transcriptions","title":"Audio Transcriptions","text":""},{"location":"server/audio/#post-audiotranscriptions","title":"POST /audio/transcriptions","text":"<p>This endpoint generates transcriptions for the provided audio file.</p> <p>Request Body</p> <ul> <li><code>file</code>: The audio file to transcribe.</li> <li><code>language</code>: The language of the audio file.</li> <li><code>prompt</code>: The prompt to use for transcription.</li> <li><code>response_format</code>: The format of the transcription response.</li> <li><code>temperature</code>: The temperature to use for transcription.</li> <li><code>timestamp_granularities</code>: The timestamp granularities to use for transcription.</li> <li><code>timeout</code>: The timeout for transcription.</li> </ul> <p>Response</p> <ul> <li>A transcription object containing the transcribed text.</li> </ul> <p>Example</p> <pre><code>curl -X POST \\\n  http://localhost:8000/audio/transcriptions \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@audio_file.wav' \\\n  -F 'language=en' \\\n  -F 'prompt=Hello, world!' \\\n  -F 'response_format=json' \\\n  -F 'temperature=0.5' \\\n  -F 'timestamp_granularities=seconds' \\\n  -F 'timeout=30'\n</code></pre>"},{"location":"server/audio/#audio-translations","title":"Audio Translations","text":""},{"location":"server/audio/#post-audiotranslations","title":"POST /audio/translations","text":"<p>This endpoint generates translations for the provided audio file.</p> <p>Request Body</p> <ul> <li><code>file</code>: The audio file to translate.</li> <li><code>language</code>: The language of the audio file.</li> <li><code>prompt</code>: The prompt to use for translation.</li> <li><code>response_format</code>: The format of the translation response.</li> <li><code>temperature</code>: The temperature to use for translation.</li> <li><code>timeout</code>: The timeout for translation.</li> </ul> <p>Response</p> <ul> <li>A translation object containing the translated text.</li> </ul> <p>Example</p> <pre><code>curl -X POST \\\n  http://localhost:8000/audio/translations \\\n  -H 'Content-Type: multipart/form-data' \\\n  -F 'file=@audio_file.wav' \\\n  -F 'language=en' \\\n  -F 'prompt=Hello, world!' \\\n  -F 'response_format=json' \\\n  -F 'temperature=0.5' \\\n  -F 'timeout=30'\n</code></pre>"},{"location":"server/audio/#flowchart","title":"Flowchart","text":"<pre><code>graph TD\n    A[Client] --&gt;|Request| B[API Router]\n    B --&gt;|Audio Speech| C[Audio Speech Handler]\n    B --&gt;|Audio Transcriptions| D[Audio Transcriptions Handler]\n    B --&gt;|Audio Translations| E[Audio Translations Handler]\n    C --&gt;|Streaming Response| A\n    D --&gt;|Transcription Response| A\n    E --&gt;|Translation Response| A</code></pre>"},{"location":"server/audio/#conclusion","title":"Conclusion","text":"<p>The <code>languru.server.api.v1.audio</code> module provides a set of RESTful API routers for handling audio-related requests and responses. This documentation aims to provide a clear and concise guide on how to use these APIs effectively. By following this guide, developers can easily integrate the <code>languru.server.api.v1.audio</code> module into their applications and utilize its features.</p>"},{"location":"server/chat/","title":"Chat API","text":""},{"location":"server/chat/#introduction","title":"Introduction","text":"<p>The <code>languru.server.api.v1.chat</code> module provides RESTful API routers for handling chat-related functionalities, specifically focusing on chat completions. This documentation aims to provide a clear and concise guide on how to use the module's APIs effectively.</p>"},{"location":"server/chat/#module-overview","title":"Module Overview","text":"<p>The <code>languru.server.api.v1.chat</code> module is designed to handle chat completion requests. It utilizes the OpenAI client to generate responses based on the provided chat completion requests. The module supports both normal and streaming chat completion requests.</p>"},{"location":"server/chat/#key-features","title":"Key Features","text":"<ul> <li>Normal Chat Completion: Handles standard chat completion requests, returning a complete response.</li> <li>Streaming Chat Completion: Supports streaming chat completion requests, providing a continuous stream of responses.</li> </ul>"},{"location":"server/chat/#api-endpoints","title":"API Endpoints","text":""},{"location":"server/chat/#chatcompletions","title":"<code>/chat/completions</code>","text":"<p>This endpoint handles chat completion requests. It accepts a <code>ChatCompletionRequest</code> object and returns a <code>ChatCompletion</code> or <code>StreamingResponse</code> based on the request type.</p>"},{"location":"server/chat/#request-body","title":"Request Body","text":"<pre><code>{\n    \"model\": \"string\",\n    \"prompt\": \"string\",\n    \"max_tokens\": \"integer\",\n    \"temperature\": \"number\",\n    \"stream\": \"boolean\"\n}\n</code></pre>"},{"location":"server/chat/#response","title":"Response","text":"<ul> <li>Normal Response: A <code>ChatCompletion</code> object containing the generated response.</li> <li>Streaming Response: A <code>StreamingResponse</code> object providing a continuous stream of responses.</li> </ul>"},{"location":"server/chat/#usage-examples","title":"Usage Examples","text":""},{"location":"server/chat/#normal-chat-completion-request","title":"Normal Chat Completion Request","text":"<pre><code>curl -X POST \\\n    http://localhost:8000/chat/completions \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\": \"gpt-3.5-turbo\", \"prompt\": \"Hello, how are you?\", \"max_tokens\": 100, \"temperature\": 0.7, \"stream\": false}'\n</code></pre>"},{"location":"server/chat/#streaming-chat-completion-request","title":"Streaming Chat Completion Request","text":"<pre><code>curl -X POST \\\n    http://localhost:8000/chat/completions \\\n    -H 'Content-Type: application/json' \\\n    -d '{\"model\": \"gpt-3.5-turbo\", \"prompt\": \"Hello, how are you?\", \"max_tokens\": 100, \"temperature\": 0.7, \"stream\": true}'\n</code></pre>"},{"location":"server/chat/#control-flow","title":"Control Flow","text":"<p>The control flow of the module can be visualized as follows:</p> <pre><code>graph TD\n    A[Receive Request] --&gt; |Normal Request| B[Handle Normal Request]\n    A --&gt; |Streaming Request| C[Handle Streaming Request]\n    B --&gt; D[Return ChatCompletion]\n    C --&gt; E[Return StreamingResponse]</code></pre>"},{"location":"server/chat/#conclusion","title":"Conclusion","text":"<p>The <code>languru.server.api.v1.chat</code> module provides a robust and flexible way to handle chat completion requests. By following this documentation, developers can easily integrate the module into their applications and utilize its RESTful APIs effectively.</p>"}]}